{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain openai pypdf chromadb tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"./LawsofTheGame2023_24.pdf\")\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "230"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_pages = pages[10:200] # Just the relevant pages\n",
    "page = trimmed_pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      " Football is the greatest sport on earth. It is played on every continent, in every \n",
      "country and at many different levels. The fact that the Laws of the Game are the \n",
      "same for all football throughout the world, from the FIFA World Cup™ through \n",
      "to a game between young children in a remote village, is a considerable \n",
      "strength which must continue to be harnessed for the good of football \n",
      "everywhere.\n",
      " Football must have Laws which keep the game fair – this is a crucial foundation \n",
      "of the ‘beauti\n"
     ]
    }
   ],
   "source": [
    "print(page.page_content[0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.split_documents(trimmed_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of trimmed_pages:  190\n",
      "Length of splits:  246\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of trimmed_pages: \", len(trimmed_pages))\n",
    "print(\"Length of splits: \", len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "Let's take our splits and embed them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = 'docs_2023_24/chroma/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing the Conversational Agent\n",
    "Our conversational agent needs a Chat LLM, conversational memory, and a RetrievalQA chain to initialize. We create these using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "# chat completion llm\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    temperature=0.0\n",
    ")\n",
    "# conversational memory\n",
    "conversational_memory = ConversationBufferWindowMemory(\n",
    "    memory_key='chat_history',\n",
    "    k=5,\n",
    "    return_messages=True\n",
    ")\n",
    "# retrieval qa chain\n",
    "convo_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    memory=conversational_memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "unique_id = uuid4().hex[0:8]\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"RefGPT_2023_24_RAG_Chromadb - {unique_id}\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")  # Update to your API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'If a referee awards a penalty kick for an offense where a defending team player challenges an opponent for the ball, excluding holding, pulling, pushing, or any action that prevents the opponent from playing the ball, the appropriate sanction is a caution (yellow card) for the offending player.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is the coorect sanction if a referee awards a penalty kick for an offence which involved a defending team player challenging an opponent for the ball (excluding holding, pulling, pushing, no possibility to play the ball etc.)?\"\n",
    "result = convo_chain({\"question\": question})\n",
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
